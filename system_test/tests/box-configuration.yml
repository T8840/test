- name: coyote
  title: Box CDH Tests
- name: Hadoop Tutorial
  skip: _hadoop_tutorial_
  entries:
    - name: Clone Landoop Intro Repo
      command: git clone https://gitlab.com/landoop/landoop-intro.git
      workdir: /home/coyote

    - name: hdfs put
      command: hadoop fs -put ../README.md input.md
      workdir: /home/coyote/landoop-intro/Hadoop-101

    - name: wordcount example
      command: hadoop jar hadoop-examples.jar wordcount input.md results
      workdir: /home/coyote/landoop-intro/Hadoop-101

    - name: build code
      command: hadoop com.sun.tools.javac.Main WordCount.java
      workdir: /home/coyote/landoop-intro/Hadoop-102

    - name: create jar
      command: jar cf wc.jar WordCount.class WordCount$IntSumReducer.class WordCount$TokenizerMapper.class
      workdir: /home/coyote/landoop-intro/Hadoop-102

    - name: run jar
      command: hadoop jar wc.jar WordCount input.md results-2
      workdir: /home/coyote/landoop-intro/Hadoop-102

    - command: hadoop fs -rm -r input.md results results-2
      nolog: true

    - command: rm -rf /home/coyote/landoop-intro
      nolog: true

- name: Sqoop Tutorial
  skip: _sqoop_tutorial_
  entries:
    - name: import
      command: |
        sqoop-import --connect jdbc:mysql://mariadb.landoop.com/northwind
                     --username data --password base
                     --table Orders --m 1

    - command: hadoop fs -rm -r Orders
      nolog: true

    - name: hive create db
      command: hive -e "CREATE DATABASE coyote_tutorial"

    - name: import into hive
      command: |
        sqoop import -jt local --connect jdbc:mysql://mariadb.landoop.com/northwind
                     --username data --password base
                     --query 'SELECT * FROM Orders WHERE $CONDITIONS' --m 1
                     --target-dir temp-orders --create-hive-table
                     --hive-table "coyote_tutorial.orders" --hive-import
                     --hive-overwrite --create-hive-table

    - name: drop hive database
      command: hive -e "USE coyote_tutorial; DROP TABLE orders; DROP DATABASE coyote_tutorial"

- name: Kafka Tests
  skip: _kafka_tests_
  entries:
    - name: Cloudera Env Vars (no fail)
      command: |
        bash -c '
        echo -e "\
        Zookeeper (ZK) $ZK\n\
        Broker List (BL) $BL\n\
        Bootstrap Server (BS) $BS\n\
        JAAS Login Configuration File (JAAS) $JAAS\n\
        producer.properties path (PROD_PR) $PROD_PR\n\
        consumer.properties path (CONS_PR) $CONS_PR\n\
        KAFKA_OPTS (KAFKA_OPTS) $KAFKA_OPTS"'

    - name: Cloudera Perf Test (Kerberos)
      command: |
        kafka-producer-perf-test --topic test-rep-one --throughput 100000 --record-size 1000 --num-records 1000000
        --producer-props bootstrap.servers="cloudera.landoop.com:9092"
#        --producer-props bootstrap.servers="cloudera.landoop.com:9092" security.protocol=SASL_PLAINTEXT

    - name: Kafka Perf Test (Non-Kerberos)
      command: |
        kafka-producer-perf-test --topic test-rep-one --throughput 100000 --record-size 1000 --num-records 1000000
        --producer-props bootstrap.servers="cloudera.landoop.com:29092"

- name: Spark Tests
  skip: _spark_tests_
  entries:
    - command: mkdir -p coyote-spark-test-app/src/main/scala
      nolog: true
    - name: Create Test Scala App
      command: tee coyote-spark-test-app/src/main/scala/CoyoteTestApp.scala
      stdin: |
        /* SimpleApp.scala */
        import org.apache.spark.SparkContext
        import org.apache.spark.SparkContext._
        import org.apache.spark.SparkConf

        object CoyoteTestApp {
          def main(args: Array[String]) {
            val logFile = "CoyoteTestApp.scala" // file should be in HDFS
            val conf = new SparkConf().setAppName("Coyote Application")
            val sc = new SparkContext(conf)
            val logData = sc.textFile(logFile, 2).cache()
            val numAs = logData.filter(line => line.contains("a")).count()
            val numBs = logData.filter(line => line.contains("b")).count()
            val lineCount = logData.count()
            println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))
            println("Linecount: %s".format(lineCount))
            sc.parallelize(List(("Linecount: %s".format(lineCount))),1).saveAsTextFile("spark-results.txt")
          }
        }
    - name: Create Sbt File
      command: tee coyote-spark-test-app/main.sbt
      stdin: |
        name := "Coyote Test Project"
        version := "1.0"
        scalaVersion := "2.11.7"
        libraryDependencies += "org.apache.spark" %% "spark-core" % "1.6.2"
    - name: Put into HDFS a Test File
      command: hadoop fs -put -f coyote-spark-test-app/src/main/scala/CoyoteTestApp.scala
    - name: Build SBT Package
      command: sbt package
      workdir: coyote-spark-test-app
    - name: Spark-submit Application (Local)
      command: |
        spark-submit
          --class "CoyoteTestApp"
          --master local[4]
          coyote-spark-test-app/target/scala-2.11/coyote-test-project_2.11-1.0.jar
      stdout_has: [ 'Linecount: 19' ]
    - name: Verify via HDFS
      command: hadoop fs -cat spark-results.txt/part-00000
      stdout_has: [ 'Linecount: 19' ]
    - command: hadoop fs -rm -r -f spark-results.txt
      nolog: true
    - name: Spark-submit Application (CDH Cluster)
      command: |
        spark-submit
          --class "CoyoteTestApp"
          --master yarn
          --deploy-mode cluster
          coyote-spark-test-app/target/scala-2.11/coyote-test-project_2.11-1.0.jar
    - name: Verify via HDFS
      command: hadoop fs -cat spark-results.txt/part-00000
      stdout_has: [ 'Linecount: 19' ]
    - command: hadoop fs -rm -r -f spark-results.txt
      nolog: true
    - command: rm -rf coyote-spark-test-app
      nolog: true
